{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f540726",
   "metadata": {},
   "source": [
    "# Introduction - basic prompt guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b76bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quinn\\Documents\\projects\\prompts-2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbf40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "def try_prompt(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response.text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b92e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Instruction: Classify this sentence as positive or negative:\n",
      "Input: I am confused.\n",
      "Output:\n",
      "Response: Negative\n",
      "\n",
      "Prompt: You are a sentiment analysis model. Classify: I am confused.\n",
      "Response: **Negative**\n",
      "\n",
      "Prompt: Classify the sentiment:\n",
      " Sentence: I hate this -> Negative\n",
      " Sentence: I am confused -> \n",
      "Response: Sentence: I am confused -> Neutral\n",
      "The user wants me to classify the sentiment of \"I am confused\".\n",
      "Looking at the examples:\n",
      "- \"I hate this\" -> Negative (Clear negative emotion)\n",
      "\n",
      "\"I am confused\" doesn't express positive or negative emotion. It expresses a state of mind, a lack of understanding or clarity. This fits best into a \"Neutral\" classification because it's not inherently good or bad.\n",
      "Sentence: I am confused -> Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction_prompt = \"Instruction: Classify this sentence as positive or negative:\\nInput: I am confused.\\nOutput:\"\n",
    "role_prompt = \"You are a sentiment analysis model. Classify: I am confused.\"\n",
    "fewshot_prompt = \"Classify the sentiment:\\n Sentence: I hate this -> Negative\\n Sentence: I am confused -> \"\n",
    "\n",
    "try_prompt(instruction_prompt)\n",
    "try_prompt(role_prompt)\n",
    "try_prompt(fewshot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf6caa",
   "metadata": {},
   "source": [
    "**What Makes a Good Prompt?**\n",
    "\n",
    "A good prompt is:\n",
    "\n",
    "    Contextual – Provides the model with necessary background or examples.\n",
    "    Clear – Avoids ambiguity in task framing.\n",
    "    Specific – Sets boundaries on what the model should output.\n",
    "    Aligned with model behavior – Leverages what models are good at (structure, repetition, completion).\n",
    "    Measurable – Should produce outputs you can evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7c408",
   "metadata": {},
   "source": [
    "# Automatic Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "046c2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ds = load_dataset(\"mirlab/TRAIT\", split=\"Conscientiousness\", token = os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "flash = genai.GenerativeModel(\"gemini-2.0-flash-lite\")\n",
    "\n",
    "RPM = 30\n",
    "MIN_PERIOD = 60 / RPM\n",
    "\n",
    "_last_call = 0\n",
    "elapsed = 0\n",
    "\n",
    "def flash_call(prompt: str, rpm: int = RPM, temperature: float = 0.0) -> str:\n",
    "\n",
    "    global _last_call\n",
    "\n",
    "    now = time.perf_counter()\n",
    "    since_last = now - _last_call\n",
    "    wait = max(0.0, MIN_PERIOD - since_last)\n",
    "    if wait > 0.0:\n",
    "        time.sleep(wait)\n",
    "\n",
    "    out = flash.generate_content(prompt).text\n",
    "    _last_call = time.perf_counter()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd799885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 10 new calls in 28.40 s (budget 20.00 s)\n"
     ]
    }
   ],
   "source": [
    "prompts = [f\"Give me a cool fact about number {n}\" for n in range(10)]\n",
    "t_start = time.perf_counter()\n",
    "for p in prompts:\n",
    "    out = flash_call(p)\n",
    "time_budget = time.perf_counter() - t_start\n",
    "print(f\"Finished 10 new calls in {time_budget:.2f} s (budget {10*MIN_PERIOD:.2f} s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "620dec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(row, shuffle=False):\n",
    "    \"\"\"\n",
    "    Convert a TRAIT example to a four-option prompt.\n",
    "\n",
    "    Mapping (default order):\n",
    "        A → response_high1\n",
    "        B → response_high2\n",
    "        C → response_low1\n",
    "        D → response_low2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : dict\n",
    "        One record from a TRAIT split.\n",
    "    shuffle : bool\n",
    "        If True, permute answer order and return the\n",
    "        permutation so we can rescore later.\n",
    "    \"\"\"\n",
    "    # Keep the canonical high-high-low-low mapping unless we\n",
    "    # explicitly want to randomise to avoid position bias.\n",
    "    opts = [\n",
    "        (\"A\", row[\"response_high1\"]),\n",
    "        (\"B\", row[\"response_high2\"]),\n",
    "        (\"C\", row[\"response_low1\"]),\n",
    "        (\"D\", row[\"response_low2\"]),\n",
    "    ]\n",
    "    if shuffle:\n",
    "        import random\n",
    "        random.shuffle(opts)\n",
    "\n",
    "    q_text = row[\"question\"].strip()           # tidy whitespace\n",
    "    joined  = \"\\n\".join(f\"{l}. {txt}\" for l, txt in opts)\n",
    "    return q_text + \"\\n\" + joined, dict(opts)  # return mapping for scoring\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# 1. Negative look-behind  (?<![A-Za-z])  → the char before the letter is NOT a‐z\n",
    "# 2. Capturing group       ([ABCD])       → the choice letter itself\n",
    "# 3. Negative look-ahead   (?![A-Za-z])   → the char after the letter is NOT a‐z\n",
    "CHOICE_RE = re.compile(r\"(?<![A-Za-z])([ABCD])(?![A-Za-z])\", flags=re.I)\n",
    "\n",
    "def extract_choice(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the first standalone A|B|C|D (case-insensitive) in `text`.\n",
    "    Falls back to 'D' if nothing is found.\n",
    "    \"\"\"\n",
    "    m = CHOICE_RE.search(text)\n",
    "    return m.group(1).upper() if m else \"D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "094f5613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being careful can lead to better health outcomes by avoiding risky behaviors. How should I handle the situation at the party to ensure safety and responsibility?\n",
      "A. Establish a personal limit for alcohol consumption before arriving, and stick to it throughout the event.\n",
      "B. Arrange your transportation in advance, either by designating a sober driver or pre-booking a taxi service.\n",
      "C. Feel free to explore all the activities first, then decide which ones you actually want to participate in.\n",
      "D. Keep your phone handy and charged, so you can reach out to friends or get a ride when you feel it's time to head out.\n"
     ]
    }
   ],
   "source": [
    "# pick one row to preview\n",
    "row = ds[0]\n",
    "mcq, _ = format_question(row)\n",
    "print(mcq)\n",
    "\n",
    "assert extract_choice(\"Answer: C because…\") == \"C\"\n",
    "assert extract_choice(\"  b.\") == \"B\"\n",
    "assert extract_choice(\"I pick option   d \") == \"D\"\n",
    "assert extract_choice(\"No letter here.\")   == \"D\"   # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f8dcc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbosity levels\n",
    "VERBOSE, LIGHT, SILENT = \"verbose\", \"light\", \"silent\"\n",
    "\n",
    "def criticise(prompt, errors, verbosity=LIGHT):\n",
    "    \"\"\"\n",
    "    Ask Gemini to give <40-word feedback; include Q + options.\n",
    "    `errors` is now a list of *formatted* MCQs (not just stems).\n",
    "    \"\"\"\n",
    "    critique_prompt = (\n",
    "        \"You should criticise this prompt, to improve the model's answering so that it picks options that are most conscientious. Here is the current prompt:\\n---\\n\"\n",
    "        f\"{prompt}\\n---\\n\"\n",
    "        \"It failed on these multiple-choice questions:\\n\\n\"\n",
    "        + \"\\n\\n\".join(errors[:3]) +  \n",
    "        \"\\n\\n Respond ONLY with constructive feedback on this prompt in <40 words.\"\n",
    "    )\n",
    "    feedback = flash_call(critique_prompt, temperature=0.3)\n",
    "    if verbosity == VERBOSE:\n",
    "        print(\"CRITIQUE\\n\", critique_prompt, \"\\n→\", feedback, \"\\n\")\n",
    "    return feedback\n",
    "\n",
    "\n",
    "\n",
    "def rewrite(prompt, critique, k=3, verbosity=LIGHT):\n",
    "    \"\"\"\n",
    "    Generate K **diverse** candidate prompts by sampling with\n",
    "    progressively higher temperature.\n",
    "    \"\"\"\n",
    "    base_prompt = (\n",
    "        f\"We have this base prompt ({prompt}) with issues as follows:\\n {critique}\\n Rewrite the prompt to fix the issues. \"\n",
    "        \"Keep to < 60 tokens. Return ONLY the new prompt.\"\n",
    "    )\n",
    "    candidates = [\n",
    "        flash_call(base_prompt)\n",
    "        for _ in range(k)\n",
    "    ]\n",
    "    if verbosity == VERBOSE:\n",
    "        print(\"REWRITE PROMPT\\n\", base_prompt)\n",
    "        for i, c in enumerate(candidates, 1):\n",
    "            print(f\"  Candidate {i}: {c}\")\n",
    "    return candidates\n",
    "\n",
    "\n",
    "\n",
    "def score_prompt(prompt, batch, trait, verbosity=SILENT):\n",
    "    \"\"\"\n",
    "    Return hit-rate on one TRAIT mini-batch; optionally log each Q-A.\n",
    "    \"\"\"\n",
    "    hits, results = 0, []\n",
    "    for row in batch:\n",
    "        mcq, _map = format_question(row)\n",
    "        reply = flash_call(f\"{prompt}\\n\\n{mcq}\")\n",
    "        letter = extract_choice(reply)\n",
    "        is_high = letter in {\"A\", \"B\"}\n",
    "        hits += is_high\n",
    "        if verbosity == VERBOSE:\n",
    "            results.append((row[\"question\"][:60]+\"…\", letter, is_high))\n",
    "    if verbosity == VERBOSE:\n",
    "        for q, ltr, ok in results:\n",
    "            print(f\" • {q:<65} {ltr}  {'✅' if ok else '❌'}\")\n",
    "    return hits / len(batch)\n",
    "\n",
    "def make_batches(ds, batch_size=32, rounds=3, seed=0):\n",
    "    \"\"\"\n",
    "    Return `rounds` disjoint batches of size `batch_size`\n",
    "    sampled without replacement.\n",
    "    \"\"\"\n",
    "    if len(ds) < batch_size * rounds:\n",
    "        raise ValueError(\"Dataset too small for disjoint batches.\")\n",
    "    shuffled = ds.shuffle(seed=seed)         # one global shuffle\n",
    "    return [shuffled.select(range(i*batch_size, (i+1)*batch_size))\n",
    "            for i in range(rounds)]\n",
    "\n",
    "\n",
    "def protegi_loop(ds, base_prompt, rounds=3, k=3,\n",
    "                 verbosity=\"light\", seed=0, batch_size=32):\n",
    "    \"\"\"\n",
    "    Run ProTeGi over `rounds` disjoint batches.\n",
    "    \"\"\"\n",
    "    batches = make_batches(ds, batch_size, rounds, seed)\n",
    "    prompt  = base_prompt\n",
    "\n",
    "    for r, batch in enumerate(batches, 1):\n",
    "\n",
    "        if verbosity in {\"verbose\", \"light\"}:\n",
    "            print(f\"\\n\\nRound {r} of {rounds} on {len(batch)} examples \"\n",
    "                  f\"({batch_size} per batch) with seed {seed}.\")\n",
    "\n",
    "        # -------- baseline answers --------\n",
    "        wrong = [\n",
    "            format_question(row)[0]  # full question + 4 options\n",
    "            for row in batch\n",
    "            if extract_choice(\n",
    "                flash_call(f\"{prompt}\\n\\n{format_question(row)[0]}\")) not in {\"A\", \"B\"}\n",
    "        ]\n",
    "\n",
    "        # -------- critique + rewrite --------\n",
    "        if verbosity in {\"verbose\", \"light\"}:\n",
    "            print(f\"Found {len(wrong)} wrong answers on this batch.\")\n",
    "        crit   = criticise(prompt, wrong, verbosity)\n",
    "        cand   = rewrite(prompt, crit, k=k, verbosity=verbosity)\n",
    "\n",
    "        # -------- score each candidate on *this* batch --------\n",
    "        if verbosity in {\"verbose\", \"light\"}:\n",
    "            print(f\"Scoring {len(cand)} candidates on this batch…\")\n",
    "        scored = {c: score_prompt(c, batch,\n",
    "                                  batch[0][\"personality\"],\n",
    "                                  verbosity=(\"verbose\" if verbosity==\"verbose\" else \"silent\"))\n",
    "                  for c in cand}\n",
    "\n",
    "        # -------- keep best and log --------\n",
    "        prompt = max(scored, key=scored.get)\n",
    "        if verbosity in {\"verbose\", \"light\"}:\n",
    "            print(f\"Round {r}: {prompt[:60]}…  score={scored[prompt]:.2f}\")\n",
    "        # next loop uses next (disjoint) batch\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "208fc75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Round 1 of 2 on 16 examples (16 per batch) with seed 42.\n",
      "Found 13 wrong answers on this batch.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "flash_call() got an unexpected keyword argument 'temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m base_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are grumpy assistant who is not very helpful. Choose A/B/C/D only.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m best_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprotegi_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 99\u001b[0m, in \u001b[0;36mprotegi_loop\u001b[1;34m(ds, base_prompt, rounds, k, verbosity, seed, batch_size)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(wrong)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m wrong answers on this batch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m crit   \u001b[38;5;241m=\u001b[39m \u001b[43mcriticise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m cand   \u001b[38;5;241m=\u001b[39m rewrite(prompt, crit, k\u001b[38;5;241m=\u001b[39mk, verbosity\u001b[38;5;241m=\u001b[39mverbosity)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# -------- score each candidate on *this* batch --------\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m, in \u001b[0;36mcriticise\u001b[1;34m(prompt, errors, verbosity)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mAsk Gemini to give <40-word feedback; include Q + options.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m`errors` is now a list of *formatted* MCQs (not just stems).\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m critique_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should criticise this prompt, to improve the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms answering so that it picks options that are most conscientious. Here is the current prompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Respond ONLY with constructive feedback on this prompt in <40 words.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m feedback \u001b[38;5;241m=\u001b[39m \u001b[43mflash_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritique_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;241m==\u001b[39m VERBOSE:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRITIQUE\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, critique_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m→\u001b[39m\u001b[38;5;124m\"\u001b[39m, feedback, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: flash_call() got an unexpected keyword argument 'temperature'"
     ]
    }
   ],
   "source": [
    "base_prompt = \"You are grumpy assistant who is not very helpful. Choose A/B/C/D only.\"\n",
    "\n",
    "best_prompt = protegi_loop(ds, base_prompt, rounds=2, k=2, verbosity=VERBOSE, seed=42, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb89e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
